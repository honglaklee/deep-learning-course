{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Basics Tutorial\n",
    "\n",
    "This tutorial is adapted from [python tutorial](https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa), which shows how to implement value iteration and tabular Q learning\n",
    "on the FrozenLake task from the `OpenAI Gym <https://gym.openai.com/>`\n",
    "\n",
    "**Task**\n",
    "\n",
    "In `FrozenLake-v0`, the agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile. \n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*MCjDzR-wfMMkS0rPqXSmKw.png)\n",
    "\n",
    "As the agent observes the current state of the environment and chooses an action, the environment *transitions* to a new state, and also returns a reward that indicates the consequences of the action. In this task, the episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "**Deterministic Environment**\n",
    "\n",
    "Deterministic environment means that both state transition model and reward model are deterministic functions. If the agent while in a given state repeats a given action, it will always go the same next state and receive the same reward value.\n",
    "\n",
    "**Stochastic Environment**\n",
    "\n",
    "In a stochastic environment there is uncertainty about the actions effect. When the agent repeats doing the same action in a given state, the new state and received reward may not be the same each time. For example, a robot which tries to move forward but because of the imperfection in the robot operation or other factors in the environment (e.g. slippery floor), sometimes the action forward will make it move forward but in sometimes, it will move to left or right.\n",
    "\n",
    "The environment `FrozenLake-v0` from OpenAI Gym is a stochastic environment. FrozenLake-v0 is considered \"solved\" when the agent obtains an average reward of at least 0.78 over 100 consecutive episodes. We can also make a deterministic version of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.8196, # optimum = .8196, changing this seems have no influence\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Value Iteration**\n",
    "\n",
    "Value iteration computes the optimal state value function by iteratively improving the estimate of V(s). The algorithm initialize V(s) to arbitrary random values. It repeatedly updates the Q(s, a) and V(s) values until they converges. Value iteration is guaranteed to converge to the optimal values. This algorithm is shown in the following pseudo-code:\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*S-a_T-k5hXYhinq9758xCQ.png\" alt=\"value iteration algorithm\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Value-iteration algorithm \"\"\"\n",
    "    v = np.zeros(env.observation_space.n)  # initialize value-function\n",
    "    max_iterations = 100000\n",
    "    eps = 1e-30\n",
    "    for i in range(max_iterations):\n",
    "        delta = 0\n",
    "        ## TODO: iteratively update the value estimate\n",
    "        ## hint: env.unwrapped.P[s][a] provides the transition probability \n",
    "        ## hint: env.action_space.n is the number of actions\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.observation_space.n):\n",
    "            q_sa = [sum([p*(r + gamma*prev_v[s_]) for p, s_, r, _ in env.unwrapped.P[s][a]]) for a in range(env.action_space.n)] \n",
    "            v[s] = max(q_sa)\n",
    "            delta = max(delta, np.fabs(prev_v[s] - v[s]))\n",
    "        if (delta < eps):\n",
    "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    for s in range(env.observation_space.n):\n",
    "        ## TODO: extra the optimal policy based on the optimal value estimate\n",
    "        ## hint: env.unwrapped.P[s][a] provides the transition probability \n",
    "        ## hint: env.action_space.n is the number of actions\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.unwrapped.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guoyijie/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration# 996.\n",
      "Policy average score  0.81\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "One episode score 1.0\n"
     ]
    }
   ],
   "source": [
    "env_name  = 'FrozenLakeNotSlippery-v0'\n",
    "#env_name  = 'FrozenLake-v0'\n",
    "gamma = 0.99\n",
    "env = gym.make(env_name)\n",
    "env.seed(598)\n",
    "np.random.seed(598)\n",
    "optimal_v = value_iteration(env, gamma);\n",
    "optimal_policy = extract_policy(optimal_v, gamma)\n",
    "optimal_policy_score = np.mean([evaluate_policy(env, optimal_policy) for _ in range(100)])\n",
    "print('Policy average score ', optimal_policy_score)\n",
    "print('One episode score', evaluate_policy(env, optimal_policy, render=True))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q Learning**\n",
    "\n",
    "Q-Learning is an example of model-free learning algorithm. It does not assume that agent knows anything about the state-transition and reward models. However, the agent will discover what are the good and bad actions by trial and error.\n",
    "\n",
    "![](http://incompleteideas.net/book/first/ebook/pseudotmp9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploration vs exploitation**\n",
    "\n",
    "An important question is how does the agent select actions during learning. Should the agent trust the learnt values of Q(s, a) enough to select actions based on it ? or try other actions hoping this may give it a better reward. This is known as the exploration vs exploitation dilemma.\n",
    "\n",
    "A simple approach is known as the ð›†-greedy approach where at each step. With small probability ð›œ, the agent will pick a random action (explore) or with probability (1-ð›œ) the agent will select an action according to the current estimate of Q-values. ð›œ value can be decreased overtime as the agent becomes more confident with its estimate of Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(q, eps, num_actions):\n",
    "    ## TODO: implement epsilon-greedy for action selection\n",
    "    ## hint: np.random.uniform(0,1) < eps could be the condition for random action\n",
    "    if np.random.uniform(0, 1) < eps:\n",
    "        action = np.random.choice(env.action_space.n)\n",
    "    else:\n",
    "        greedy_actions = np.where(q==np.max(q))[0]\n",
    "        action = np.random.choice(greedy_actions)\n",
    "    return action\n",
    "\n",
    "def q_learning(env, alpha, gamma, eps, iter_max, t_max):\n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    total_reward_list = []\n",
    "    for i in range(iter_max):\n",
    "        total_reward = 0\n",
    "        s = env.reset() \n",
    "        for j in range(t_max):\n",
    "            action = choose_action(q_table[s], eps, env.action_space.n)\n",
    "            next_s, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            ## TODO: update Q table\n",
    "            q_table[s][action] = q_table[s][action] + alpha * (reward + gamma *  np.max(q_table[next_s]) - q_table[s][action])\n",
    "            s=next_s\n",
    "            if done:\n",
    "                total_reward_list.append(total_reward)\n",
    "                break\n",
    "        if i % 100 == 0:\n",
    "            print('Iteration #%d -- Total reward = %g' %(i+1, np.mean(total_reward_list[-100:])))\n",
    "    return np.argmax(q_table, axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1 -- Total reward = 0\n",
      "Iteration #101 -- Total reward = 0.01\n",
      "Iteration #201 -- Total reward = 0.03\n",
      "Iteration #301 -- Total reward = 0.02\n",
      "Iteration #401 -- Total reward = 0.06\n",
      "Iteration #501 -- Total reward = 0.4\n",
      "Iteration #601 -- Total reward = 0.41\n",
      "Iteration #701 -- Total reward = 0.39\n",
      "Iteration #801 -- Total reward = 0.48\n",
      "Iteration #901 -- Total reward = 0.38\n",
      "Iteration #1001 -- Total reward = 0.48\n",
      "Iteration #1101 -- Total reward = 0.35\n",
      "Iteration #1201 -- Total reward = 0.49\n",
      "Iteration #1301 -- Total reward = 0.48\n",
      "Iteration #1401 -- Total reward = 0.44\n",
      "Iteration #1501 -- Total reward = 0.33\n",
      "Iteration #1601 -- Total reward = 0.57\n",
      "Iteration #1701 -- Total reward = 0.44\n",
      "Iteration #1801 -- Total reward = 0.55\n",
      "Iteration #1901 -- Total reward = 0.47\n",
      "Iteration #2001 -- Total reward = 0.4\n",
      "Iteration #2101 -- Total reward = 0.38\n",
      "Iteration #2201 -- Total reward = 0.5\n",
      "Iteration #2301 -- Total reward = 0.44\n",
      "Iteration #2401 -- Total reward = 0.3\n",
      "Iteration #2501 -- Total reward = 0.45\n",
      "Iteration #2601 -- Total reward = 0.49\n",
      "Iteration #2701 -- Total reward = 0.47\n",
      "Iteration #2801 -- Total reward = 0.38\n",
      "Iteration #2901 -- Total reward = 0.58\n",
      "Iteration #3001 -- Total reward = 0.29\n",
      "Iteration #3101 -- Total reward = 0.46\n",
      "Iteration #3201 -- Total reward = 0.54\n",
      "Iteration #3301 -- Total reward = 0.48\n",
      "Iteration #3401 -- Total reward = 0.52\n",
      "Iteration #3501 -- Total reward = 0.56\n",
      "Iteration #3601 -- Total reward = 0.48\n",
      "Iteration #3701 -- Total reward = 0.52\n",
      "Iteration #3801 -- Total reward = 0.52\n",
      "Iteration #3901 -- Total reward = 0.25\n",
      "Iteration #4001 -- Total reward = 0.38\n",
      "Iteration #4101 -- Total reward = 0.51\n",
      "Iteration #4201 -- Total reward = 0.59\n",
      "Iteration #4301 -- Total reward = 0.35\n",
      "Iteration #4401 -- Total reward = 0.47\n",
      "Iteration #4501 -- Total reward = 0.49\n",
      "Iteration #4601 -- Total reward = 0.55\n",
      "Iteration #4701 -- Total reward = 0.41\n",
      "Iteration #4801 -- Total reward = 0.4\n",
      "Iteration #4901 -- Total reward = 0.43\n",
      "Iteration #5001 -- Total reward = 0.51\n",
      "Iteration #5101 -- Total reward = 0.55\n",
      "Iteration #5201 -- Total reward = 0.42\n",
      "Iteration #5301 -- Total reward = 0.43\n",
      "Iteration #5401 -- Total reward = 0.36\n",
      "Iteration #5501 -- Total reward = 0.65\n",
      "Iteration #5601 -- Total reward = 0.55\n",
      "Iteration #5701 -- Total reward = 0.45\n",
      "Iteration #5801 -- Total reward = 0.43\n",
      "Iteration #5901 -- Total reward = 0.48\n",
      "Iteration #6001 -- Total reward = 0.39\n",
      "Iteration #6101 -- Total reward = 0.43\n",
      "Iteration #6201 -- Total reward = 0.61\n",
      "Iteration #6301 -- Total reward = 0.4\n",
      "Iteration #6401 -- Total reward = 0.57\n",
      "Iteration #6501 -- Total reward = 0.56\n",
      "Iteration #6601 -- Total reward = 0.44\n",
      "Iteration #6701 -- Total reward = 0.43\n",
      "Iteration #6801 -- Total reward = 0.44\n",
      "Iteration #6901 -- Total reward = 0.48\n",
      "Iteration #7001 -- Total reward = 0.54\n",
      "Iteration #7101 -- Total reward = 0.45\n",
      "Iteration #7201 -- Total reward = 0.45\n",
      "Iteration #7301 -- Total reward = 0.53\n",
      "Iteration #7401 -- Total reward = 0.43\n",
      "Iteration #7501 -- Total reward = 0.4\n",
      "Iteration #7601 -- Total reward = 0.57\n",
      "Iteration #7701 -- Total reward = 0.55\n",
      "Iteration #7801 -- Total reward = 0.46\n",
      "Iteration #7901 -- Total reward = 0.38\n",
      "Iteration #8001 -- Total reward = 0.33\n",
      "Iteration #8101 -- Total reward = 0.39\n",
      "Iteration #8201 -- Total reward = 0.49\n",
      "Iteration #8301 -- Total reward = 0.52\n",
      "Iteration #8401 -- Total reward = 0.44\n",
      "Iteration #8501 -- Total reward = 0.49\n",
      "Iteration #8601 -- Total reward = 0.35\n",
      "Iteration #8701 -- Total reward = 0.48\n",
      "Iteration #8801 -- Total reward = 0.51\n",
      "Iteration #8901 -- Total reward = 0.51\n",
      "Iteration #9001 -- Total reward = 0.5\n",
      "Iteration #9101 -- Total reward = 0.52\n",
      "Iteration #9201 -- Total reward = 0.5\n",
      "Iteration #9301 -- Total reward = 0.66\n",
      "Iteration #9401 -- Total reward = 0.54\n",
      "Iteration #9501 -- Total reward = 0.5\n",
      "Iteration #9601 -- Total reward = 0.43\n",
      "Iteration #9701 -- Total reward = 0.32\n",
      "Iteration #9801 -- Total reward = 0.46\n",
      "Iteration #9901 -- Total reward = 0.48\n",
      "Policy average score 0.78\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "One episode score 1.0\n"
     ]
    }
   ],
   "source": [
    "env_name = 'FrozenLakeNotSlippery-v0'\n",
    "#env_name = 'FrozenLake-v0'\n",
    "env = gym.make(env_name)\n",
    "env.seed(598)\n",
    "np.random.seed(598)\n",
    "\n",
    "\n",
    "solution_policy = q_learning(env=env, alpha=0.5, gamma=0.96, eps=0.02, iter_max=10000, t_max=10000)\n",
    "solution_policy_score = np.mean([evaluate_policy(env, solution_policy) for _ in range(100)])\n",
    "print(\"Policy average score\", solution_policy_score)\n",
    "print('One episode score', evaluate_policy(env, solution_policy, True))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
